---
title: Introducción a la extension del API de audio
slug: Introducción_a_la_extensión_de_la_API_de_audio
tags:
  - para_revisar
  - páginas_a_traducir
translation_of: Archive/Mozilla/Introducing_the_Audio_API_Extension
---
<p>La extension de audio de la API se extiende de la especificación de HTML5 {HTMLElement("audio")}} y <a href="/es/docs/Web/HTML/Elemento/video" title=""><code>&lt;video&gt;</code></a> multimedia mediante la exposición de elementos metadata de audio y datos puros en audio. Esto permite a los usuarios visualizar los datos de audio, para procesarlos y crear nuevos datos de audio.</p>
<h2 id="Lectura_de_flujo_de_Audio">Lectura de flujo de Audio</h2>
<h3 id="El_loadedmetadata_evento">El <strong>loadedmetadata</strong> evento</h3>
<p>Cuando la metadata multimedia del elemento está disponible, se desencadena un envento <strong>loadedmetadata</strong>. Este evento tiene los siguientes atributos:</p>
<ul>
 <li><strong>mozChannels</strong>: Número de canales</li>
 <li><strong>mozSampleRate</strong>: Frecuencia de muestreo por segundo</li>
 <li><strong>mozFrameBufferLength</strong>:Número de muestras recolectadas en todos los canales</li>
</ul>
<p>Esta información es necesaria más adelante para descifrar la secuencia de datos de audio. En los siguientes ejemplos se extraen los datos de un elemento de audio:</p>
<pre class="brush: js">				&lt;!DOCTYPE html&gt;

				&lt;html&gt;

				&lt;head&gt;

					&lt;title&gt;Ejemplo JavaScript Metadata &lt;/title&gt;

				&lt;/head&gt;

					&lt;body&gt;

						&lt;audio id="audio-element"

						src="song.ogg"

						controls="true"

						style="width: 512px;"&gt;

						&lt;/audio&gt;

					    &lt;script&gt;

					function loadedMetadata() {

						channels          = audio.mozChannels;

						rate              = audio.mozSampleRate;

						frameBufferLength = audio.mozFrameBufferLength;

					      }

					var audio = document.getElementById('audio-element');

					      audio.addEventListener('loadedmetadata', loadedMetadata, false);



					    &lt;/script&gt;

					  &lt;/body&gt;

					&lt;/html&gt;

			</pre>
<h3 id="El_evento_MozAudioAvailable">El evento<strong> MozAudioAvailable </strong></h3>
<p>A medida que el audio se reproduce, los datos de muestras están disponible para la capa de audio y el buffer de audio (size definido en <strong>mozFrameBufferLength</strong>) se llenan con las muestras. Una vez que el buffer esta lleno, el evento<strong>MozAudioAvailable</strong> se activa. Por lo tanto el evento contiene las muestras primas en un perido de tiempo. Esos muestreos pueden ser reproducidos o no en el momento que se hayan reproducido el evento y no se han ajustado para la configuraciÃ³n de mudo o volumen en el elemento de multimedia. Reproducir, pausar, y la bÃºsqueda de audio tambien afecta transmision de estos datos de audio prima.</p>
<p>El evento <strong>MozAudioAvailable</strong> tiene 2 atributos:</p>
<ul>
 <li><strong>frameBuffer</strong>: Framebuffer (i.e., an array) contiene decodificado de audio datos de la muestra (i.e., floats)</li>
 <li><strong>time</strong>: Marca de tiempo de esas muestras medidas desde el principio en segundos</li>
</ul>
<p>El framebuffer contiene una serie de muestras de audio. Es importnt tener en cuenta que las muestras no son separados por canales, todos ellos son entregados juntos. Por ejemplo, para una seÃ±al de dos canales: Canal1-Ejemplo1 Canal2-Ejemplo1  Canal1-ejemplo2 Canal2-Ejemplo2 Canal1-Ejemplo3 Canal2-Ejemplo3.</p>
<p>Podemos extender el ejemplo anterior para visualizar la fecha y hora y las muestras de los dos primeros en un <a href="/es/docs/Web/HTML/Elemento/div" title='div de "division" -división . Sirve para crear secciones o agrupar contenidos.'><code>&lt;div&gt;</code></a> elemento:</p>
<pre class="brush: js">

			&lt;!DOCTYPE html&gt;

			&lt;html&gt;

				&lt;head&gt;

					&lt;title&gt;Ejemplo de visualizacion en JavaScript&lt;/title&gt;

				&lt;body&gt;



					&lt;audio id="audio-element"

					src="revolve.ogg"

					controls="true"

					style="width: 512px;"&gt;

					&lt;/audio&gt;

					&lt;pre id="raw"&gt;hello&lt;/pre&gt;

					&lt;script&gt;

				function loadedMetadata() {

					channels          = audio.mozChannels;

					rate              = audio.mozSampleRate;

					frameBufferLength = audio.mozFrameBufferLength;

				}



				function audioAvailable(event) {

					        var frameBuffer = event.frameBuffer;

					var t = event.time;





					        var text = "Ejemplo: " + t + "\n"

					        text += frameBuffer[0] + "  " + frameBuffer[1]

					        raw.innerHTML = text;

				}



					      var raw = document.getElementById('raw')

					var audio = document.getElementById('audio-element');

					audio.addEventListener('MozAudioAvailable', audioAvailable, false);

					      audio.addEventListener('loadedmetadata', loadedMetadata, false);



					&lt;/script&gt;

				&lt;/body&gt;

			&lt;/html&gt;



		</pre>
<h2 id="Creación_de_un_flujo_de_audio">Creación de un flujo de audio</h2>
<p>También es posible crear y configurar un <a href="/es/docs/Web/HTML/Elemento/audio" title="El elemento audio se usa para insertar contenido de audio en un documento HTML o XHTML. El elemento audio se agregó como parte de HTML 5."><code>&lt;audio&gt;</code></a> elemento por escrito por raw de script (i.e., sin <em>src</em> atributos. Contiene scripts que pueden especificar audios caracterÃ­sticos de los flujos, a continuaciÃ³n, escribir las muestras de audio. Los usuarios deben crear un objeto de audio y luego usar el <code>mozSetup()</code> funciÃ³n  especifica el numero de canales y frecuencia (en Hz).  Por Ejemplo:</p>
<pre class="brush: js">

			// Crear un nuevo elemento de audio

			var audioOutput = new Audio();

			// Crea un elemento de audio con 2 canales, 44.1KHz audio flujo.

			audioOutput.mozSetup(2, 44100);

		</pre>
<p>Una vez terminado, los ejemplos necesitan ser creado. Estas muestras tienen el mismo formato que los de el evento<strong>mozAudioAvailable</strong>. Luego las muestra son escritas en el audio de flujo con la funciÃ³n <code>mozWriteAudio()</code>. Es importante observar que no todas las muestras podrÃ­a obtener por escrito en el arroyo. La funciÃ³n retorna el nÃºmero de muestras por escrito, que es Ãºtil para la redacciÃ³n siguiente. Puede ver un ejemplo a continuaciÃ³n<code><span style="font-family: Verdana,Tahoma,sans-serif;">: </span></code></p>
<pre class="brush: js">

			// Escribir ejemplo usando un JS Array

			var samples = [0.242, 0.127, 0.0, -0.058, -0.242, ...];

			var numberSamplesWritten = audioOutput.mozWriteAudio(samples);



			// Escribir ejemplo usando un Typed Array

			var samples = new Float32Array([0.242, 0.127, 0.0, -0.058, -0.242, ...]);

			var numberSamplesWritten = audioOutput.mozWriteAudio(samples);

		</pre>
<p>En el siguiente ejemplo, creamos un pulso de audio:</p>
<pre class="brush: js">

			&gt;&lt;!doctype html&gt;

				&lt;html&gt;

				  &lt;head&gt;

				     &lt;title&gt;Generando audio en tiempo real&lt;/title&gt;

				  &lt;script type="text/javascript"&gt;

				     function playTone() {

				      var output = new Audio();



				      output.mozSetup(1, 44100);

				       var samples = new Float32Array(22050);

				       var len = samples.length;



				      for (var i = 0; i &lt; samples.length ; i++) {

				         samples[i] = Math.sin( i / 20 );

				       }

				              output.mozWriteAudio(samples);



				     }

				   &lt;/script&gt;

				 &lt;/head&gt;

				 &lt;body&gt;

				   &lt;p&gt;Esta demo tiene un tono de un segundo al hacer clic en el botÃ³n de abajo.&lt;/p&gt;

				   &lt;button onclick="playTone();"&gt;Play&lt;/button&gt;



				 &lt;/body&gt;

				 &lt;/html&gt;</pre>
<p>El <code>mozCurrentSampleOffset()</code> mÃ©todo da la posiciÃ³n sonora de la corriente de audio, es decir, la posiciÃ³n de la Ãºltima muestra escuchado.</p>
<pre class="brush: js">

				// Obtener la posiciÃ³n actual sonora de la secuencia de audio subyacentes, medido en las muestras.

				var currentSampleOffset = audioOutput.mozCurrentSampleOffset();

			</pre>
<p>Los datos de audio por escrito con el <code>mozWriteAudio()</code> mÃ©todo tiene que ser escrita en un intervalo regular en partes iguales, a fin de mantener un poco por delante de la muestra actual de compensaciÃ³n (offset de la muestra que estÃ¡ siendo desempeÃ±ado por el hardware se puede obtener con <code>mozCurrentSampleOffset()</code>), donde "un poco" quiere decir algo del orden de 500 ms de las muestras. Por ejemplo, si se trabaja con dos canales de 44.100 muestras por segundo, un intervalo de escritura de 100 ms, y un buffer de pre-igual a 500 ms, se podrÃ­a escribir una serie de(2 * 44100 / 10) = 8820 muestras, y un total de (currentSampleOffset + 2 * 44100 / 2).</p>
<p>TambiÃ©n es posible detectar automÃ¡ticamente la duraciÃ³n mÃ­nima de la memoria previa, de tal manera que el sonido se reproduce sin interrupciones, y el desfase entre la escritura y la reproducciÃ³n es mÃ­nimo. Para ello empezar a escribir los datos en pequeÃ±as porciones y esperar a que el valor devuelto por <code>mozCurrentSampleOffset()</code> debe ser mayor que 0.</p>
<pre class="brush: js">				var prebufferSize = sampleRate * 0.020; // Initial buffer is 20 ms

				var autoLatency = true, started = new Date().valueOf();

				...

				// Auto latency detection

				if (autoLatency) {

				prebufferSize = Math.floor(sampleRate * (new Date().valueOf() - started) / 1000);

				if (audio.mozCurrentSampleOffset()) { // Play position moved?

				autoLatency = false;

				}

			</pre>
<h2 id="El_procesamiento_de_un_audio_de_flujo">El procesamiento de un audio de flujo</h2>
<p>Desde el <strong>MozAudioAvailable</strong> evento y el<code>mozWriteAudio()</code> metodo ambos utilizan <code> Float32Array</code> valores, es posible tomar la salida de un flujo de audio y pasarlo directamente (o primer proceso y luego pasar) a un segundo. La secuencia de audio primero debe ser silenciado de manera que sÃ³lo el elemento secundario de audio que se escucha.</p>
<pre class="brush: js">

				&lt;audio id="a1"

				src="song.ogg"

				controls&gt;

				&lt;/audio&gt;

				&lt;script&gt;

				var a1 = document.getElementById('a1'),

				a2 = new Audio(),

				buffers = [];



				function loadedMetadata() {

					// Mute a1 audio.

					a1.volume = 0;

					// Configuracion a2 de ser idÃ©ntica a la a1, y jugar por ahÃ­.

					a2.mozSetup(a1.mozChannels, a1.mozSampleRate);

				}



				function audioAvailable(event) {

					// Escriba el actual framebuffer

					var frameBuffer = event.frameBuffer;

					writeAudio(frameBuffer);

				}



				a1.addEventListener('MozAudioAvailable', audioAvailable, false);

				a1.addEventListener('loadedmetadata', loadedMetadata, false);



				function writeAudio(audio) {

				buffers.push(audio);



					// Si hay buffer de datos, escribe que

					while(buffers.length &gt; 0) {

						var buffer = buffers.shift();

						var written = a2.mozWriteAudio(buffer);

						// // Si todos los datos no se ha escrito,mantener los buffers:

						if(written &lt; buffer.length) {

							buffers.unshift(buffer.slice(written));

							return;

							}

					}

				}

				&lt;/script&gt;

			</pre>
<h2 id="Ver_Tambien">Ver Tambien</h2>
<ul>
 <li><a href="/en/Creating_a_Web_based_tone_generator" title="en/Creating a Web based tone generator">Crear un generador de Web basada en el tono</a></li>
 <li><a href="/en/Visualizing_Audio_Spectrum" title="en/Visualizing Audio Spectrum"><span class="mw-headline" id="Complete_Example:_Visualizing_Audio_Spectrum">Visualizar un espectro de audio</span></a></li>
 <li><a href="/en/Displaying_the_Mozilla_logo_with_the_Audio_Samples" title="en/Displaying the Mozilla logo with the Audio Samples"><span class="mw-headline">Viendo un grÃ¡fico con las muestras de audio</span></a></li>
 <li><a href="/en/Creating_a_simple_synth" title="https://developer.mozilla.org/en/Creating_a_simple_synth">CreaciÃ³n de un synth simple (utilizando las bibliotecas existentes)</a></li>
 <li><a class="external" href="http://en.wikipedia.org/wiki/Digital_audio" title="http://en.wikipedia.org/wiki/Digital_audio">ArtÃ­culo de Wikipedia sobre audio digital</a></li>
</ul>
