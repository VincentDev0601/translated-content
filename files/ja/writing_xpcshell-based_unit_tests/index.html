---
title: xpcshellベースのユニットテスト（単体テスト）の書き方
slug: Writing_xpcshell-based_unit_tests
tags:
  - Automated testing
  - Developing Mozilla
translation_of: Mozilla/QA/Writing_xpcshell-based_unit_tests
---
<p><a href="/Ja/Xpcshell" title="ja/xpcshell">xpcshell</a> ツールはいくつかの種類の機能のテストに利用可能です。XPCOM の層で（スクリプトから利用可能なインターフェースを通じて）利用可能な物は、xpcshell でテストする事ができます。より多くの情報への手がかりを得るには<a href="/Ja/Mozilla_automated_testing" title="ja/Mozilla_automated_testing">Mozilla の自動テスト</a>および <a class="internal" href="/Special:Tags?tag=Automated+testing" title="Special:Tags?tag=Automated+testing">"automated testing" タグが指定されているページ</a>を参照してください。</p>

<h3 id="Your_first_xpcshell-based_test" name="Your_first_xpcshell-based_test">初めての xpcshell ベースのテスト</h3>

<p>初めての xpcshell ベースのテストの作り方は、簡単です。以下の内容で、<code>test_first.js</code> という名前（ユニットテストのファイル名は必ず <code>test_</code> で始めてください）のファイルを作成してください。：</p>

<pre class="eval">function run_test()
{
  // 何か複雑な処理を行い、その結果としてどのような結果が望まれているのかをここに書いてください。
  // これはただの例なので、trueを返しています。
  ok(true);
}
</pre>

<p>これは実際には何もテストしませんが、実際にどのようにテストを書けばよいのかについての考え方を示しています。テストを実行したい時には、まず初めに <a href="https://developer.mozilla.org/en/Writing_xpcshell-based_unit_tests#Adding_your_tests_to_the_xpcshell_manifest" title="en/Writing_xpcshell-based_unit_tests#Adding_your_tests_to_the_xpcshell_manifest">xpcshell.ini manifest</a> ファイルにこのテストファイル名を書く必要があります。テストファイルを既存のテストのセットの中に追加してください（例えば{{ Source("network/test/unit/") }}など。そして、 <a href="https://developer.mozilla.org/en-US/docs/Developer_Guide/mach" title="/en-US/docs/Developer_Guide/mach">mach</a> を利用してテストを実行します。</p>

<pre class="eval"><code class="language-html">$ ./mach xpcshell-test netwerk/test/</code>
...失敗または成功の旨のメッセージがコンソールに出力されます...
</pre>

<div class="note"><strong>Note:</strong>

<p>単体のテストは <a href="https://developer.mozilla.org/en-US/docs/Developer_Guide/mach" title="/en-US/docs/Developer_Guide/mach">mach</a> を使って次の様に実行します。</p>

<pre class="language-html"><code class="language-html">$ ./mach xpcshell-test <span class="tag token"><span class="tag token"><span class="punctuation token">&lt;</span>path_to_test_file</span><span class="punctuation token">&gt;</span></span></code></pre>
</div>

<h3 id="Adding_to_your_test" name="Adding_to_your_test">自分のテストを追加する</h3>

<p>The test is executed by the test harness by calling the <code>run_test</code> function defined in the file. Anything you want to test must be spawned from this function.</p>

<p>If you have a group of files in a directory and they all include some common code that needs to run before or after your tests, to set up resources for example, you can put one copy of that code in a common source file and add it to the head or tail section of the manifest.</p>

<p>Each of your test files needs to contain at least a "<code>run_test()</code>" function. If the "<code>run_test()</code>" function runs to completion without throwing an exception, the test succeeds. If the your test needs to wait for asynchronous callbacks, you can tell the test harness to not kill the test when your callback has finished (as opposed to when <code>run_test()</code> has finished) with <code>do_test_pending</code> and <code>do_test_finished</code> (see below). <code>add_test</code> and <code>run_next_test</code> provide a nice interface around that if you want to run several asynchronous tests in succession.</p>

<p>If you want to import a common module in your tests you can use <code><a class="external external-icon" rel="freelink">resource://test</a></code> to load it. This special address always resolves to the current test folder.</p>

<pre><code class="language-html">Components.utils.import("resource://test/module.jsm"); // Import module.jsm that is in the same folder as current test.</code><span style="color: rgb(0, 136, 0);">
</span></pre>

<h3 id="XPCShell_test_utility_functions" name="XPCShell_test_utility_functions">XPCShell test functions</h3>

<p>xpcshell tests have access to the following functions. They are defined in <a class="external" href="http://mxr.mozilla.org/mozilla-central/source/testing/xpcshell/head.js" title="http://mxr.mozilla.org/mozilla-central/source/testing/xpcshell/head.js">testing/xpcshell/head.js</a> and <a class="external" href="http://mxr.mozilla.org/mozilla-central/source/testing/modules/Assert.jsm" title="http://mxr.mozilla.org/mozilla-central/source/testing/modules/Assert.jsm">testing/modules/Assert.jsm</a> if you want to see how they work.</p>

<h4 id="Test_case_registration_and_execution">Test case registration and execution</h4>

<dl>
 <dt><code>add_test(<var>testFunction</var>)</code></dt>
 <dd>Add a test function to the list of tests that are to be run asynchronously. Each test function must call <code>run_next_test()</code> when it's done. <code>run_test()</code> should also call <code>run_next_test()</code> to start execution of all asynchronous test functions. In most cases, you should rather use the more readable variant <code>add_task</code>.</dd>
</dl>

<dl>
 <dt><code>run_next_test()</code></dt>
 <dd>Run the next test function from the list of asynchronous tests. Each test function must call <code>run_next_test()</code> when it's done. <code>run_test()</code> should also call <code>run_next_test()</code> to start execution of all asynchronous test functions.</dd>
</dl>

<dl>
 <dt><code>add_task(testGenerator)</code></dt>
 <dd>Add a generator to the list of tests that are to be run asynchronously. Whenever the generator <code>yield</code>s a <a href="/en-US/docs/Mozilla/JavaScript_code_modules/Promise.jsm">Promise</a>, the test runner waits until the promise is resolved or rejected before proceeding. As in <a href="/en-US/docs/Mozilla/JavaScript_code_modules/Task.jsm">Task.jsm</a>, rejected promises are converted into exceptions, and resolved promises are converted into values. <code>run_test()</code> should also call <code>run_next_test()</code> to start execution of all asynchronous test functions. The test cases must not call <code>run_next_test()</code>, it is called automatically when the task finishes.</dd>
 <dt><code>do_register_cleanup(<var>callback</var>)</code></dt>
 <dd>Executes the function <code>callback </code>after the test has finished running, regardless of whether the test passes or fails. You can use this to clean up anything that might otherwise cause problems between test runs.</dd>
 <dt><code>do_test_pending()</code></dt>
 <dd>Delay exit of the test until do_test_finished() is called. do_test_pending() may be called multiple times, and do_test_finished() must be paired with each before the unit test will exit.</dd>
 <dt><code>do_test_finished()</code></dt>
 <dd>Call this function to inform the test framework that an asynchronous operation has completed. If all asynchronous operations have completed (i.e., every do_test_pending() has been matched with a do_test_finished() in execution), then the unit test will exit.</dd>
</dl>

<h4 id="Assertions_and_logging">Assertions and logging</h4>

<dl>
 <dt><code>ok, equal, notEqual, deepEqual, notDeepEqual, strictEqual, notStrictEqual</code></dt>
 <dd>These assertion methods are provided by <a href="/en/docs/Mozilla/JavaScript_code_modules/Assert.jsm" title="/en/docs/Mozilla/JavaScript_code_modules/Assert.jsm">Assert.jsm</a>. It implements the <a href="http://wiki.commonjs.org/wiki/Unit_Testing/1.1" title="http://wiki.commonjs.org/wiki/Unit_Testing/1.1">CommonJS Unit Testing specification version 1.1</a>, which provides a basic, standardized interface for performing in-code logical assertions with optional, customizable error reporting. It is <em>highly</em> recommended to use these assertion methods, instead of the ones mentioned below.</dd>
 <dt><code>Assert.throws(<em>callback</em><var>, expectedException</var>, optional message)</code></dt>
 <dt><code>Assert.throws(<em>callback</em><var>, </var>message)</code></dt>
 <dd>Asserts that the provided callback function throws an exception. The <code>expectedException</code> argument can be an <code>Error</code> instance, or a regular expression matching part of the error message (like in <code>Assert.throws(() =&gt; a.b, /is not defined/</code>).</dd>
 <dt><code>do_check_eq(<var>a</var>, <var>b</var>)</code>{{deprecated_inline("32.0")}}</dt>
 <dd>Call this function to assert that two objects are equal (using ==). If not equal, an exception is logged and the test case is halted.</dd>
 <dt><code>do_check_neq(<var>a</var>, <var>b</var>)</code>{{deprecated_inline("32.0")}}</dt>
 <dd>Call this function to assert that two objects are not equal (using !=). If equal, an exception is logged and the test case is halted.</dd>
 <dt><code>do_check_true(<var>expr</var>)</code>{{deprecated_inline("32.0")}}</dt>
 <dd>Call this function to assert that <code>expr</code> is equal to <code>true</code>.</dd>
 <dt><code>do_check_false(<var>expr</var>)</code>{{deprecated_inline("32.0")}}</dt>
 <dd>Call this function to assert that <code>expr</code> is equal to <code>false</code>.</dd>
 <dt><code>do_check_null(<var>expr</var>)</code>{{deprecated_inline("32.0")}}</dt>
 <dd>Call this function to assert that <code>expr</code> is equal to <code>null</code>.</dd>
 <dt><code>do_print(<var>messageText</var>)</code></dt>
 <dd>Call this function to print text to the test's log file.</dd>
</dl>

<dl>
 <dt><code>do_throw(<var>messageText</var>)</code>{{deprecated_inline("32.0")}}</dt>
 <dd>Call this function to report an error and exit the test. The argument is a string that will be reported in the test's log file.</dd>
 <dd><em>Note</em>: While <code>do_throw</code> can be caught by a <code>try/catch</code> block, executing it will cause the test to fail when it completes.</dd>
</dl>

<h4 id="Environment">Environment</h4>

<dl>
 <dt><code>do_get_file(<var>testdirRelativePath</var>, <var>allowNonexistent</var>)</code></dt>
 <dd>Returns an {{ interface("nsILocalFile") }} object representing the given file (or directory) in the test directory. For example, if your test is unit/test_something.js, and you need to access unit/data/somefile, you would call <code>do_get_file('data/somefile')</code>. The given path must be delimited with forward slashes. You can use this to access test-specific auxiliary files if your test requires access to external files. Note that you can also use this function to get directories.
 <div class="note"><strong>Note:</strong> If your test needs access to one or more files that aren't in the test directory, you should install those files to the test directory in the Makefile where you specify <code>XPCSHELL_TESTS</code>. For an example, see {{ Source("netwerk/test/Makefile.in#117") }}.</div>
 </dd>
 <dt><code>do_get_profile()</code></dt>
 <dd>Registers a directory with the profile service and returns an {{ interface("nsILocalFile") }} object representing that directory. It also makes sure that the <strong>profile-change-net-teardown</strong>, <strong>profile-change-teardown</strong>, and <strong>profile-before-change</strong> <a href="/en/Observer_Notifications#Application_shutdown" title="en/Observer_Notifications#Application_shutdown">observer notifications</a> are sent before the test finishes. This is useful if the components loaded in the test observe them to do cleanup on shutdown (e.g., places).</dd>
 <dt><code>do_get_idle(</code><code>)</code></dt>
 <dd>By default XPCShell tests will disable the idle service, so that idle time will always be reported as 0. Calling this function will re-enable the service and return a handle to it; the idle time will then be correctly requested to the underlying OS. The idle-daily notification could be fired when requesting idle service. It is suggested to always get the service through this method if the test has to use idle.</dd>
 <dt><code>do_get_cwd()</code></dt>
 <dd>Returns an {{ interface("nsILocalFile") }} object representing the test directory. This is the directory containing the test file when it is currently being run. Your test can write to this directory as well as read any files located alongside your test. Your test should be careful to ensure that it will not fail if a file it intends to write already exists, however.</dd>
 <dt><code>load(<var>testdirRelativePath</var>)</code></dt>
 <dd>Imports the JavaScript file referenced by <code><var>testdirRelativePath</var></code> into the global script context, executing the code inside it. The file specified is a file within the test directory. For example, if your test is unit/test_something.js and you have another file unit/extra_helpers.js, you can load the second file from the first simply by calling <code>load('extra_helpers.js')</code>.</dd>
 <dt><code>do_load_module(<var>testdirRelativePath</var>)</code></dt>
 <dd>Calls do_get_file(<code><var>testdirRelativePath</var></code>), then registers the returned file.</dd>
</dl>

<h4 id="Utility">Utility</h4>

<dl>
 <dt><code>do_parse_document(<var>path</var>, <var>type</var>)</code></dt>
 <dd>Parses and returns a DOM document.</dd>
 <dt><code>do_execute_soon(<var>callback</var>)</code></dt>
 <dd>Executes the function <code>callback</code> on a later pass through the event loop. Use this when you want some code to execute after the current function has finished executing, but you don't care about a specific time delay. This function will automatically insert a <code>do_test_pending</code> / <code>do_test_finished</code> pair for you.</dd>
</dl>

<dl>
 <dt><code>do_timeout(<var>delay</var>, <var>fun</var>)</code></dt>
 <dd>Call this function to schedule a timeout. The given function will be called with no arguments provided after the specified delay (in milliseconds). Note that you must call <code>do_test_pending</code> so that the test isn't completed before your timer fires, and you must call <code>do_test_finished</code> when the actions you perform in the timeout complete, if you have no other functionality to test. (Note: the function argument used to be a string argument to be passed to eval, and some older branches support only a string argument or support both string and function.)</dd>
</dl>

<h3 id="Async_tests">Async tests</h3>

<p>When testing async APIs, we need to tell the test harness not to kill the test process once <code>run_test()</code> is finished, but to keep spinning the event loop until our callbacks have been called and our test has completed. This can be achieved with <code>do_test_pending()</code> and <code>do_test_finished()</code>:</p>

<pre>function run_test() {
  // Tell the harness to keep spinning the event loop at least
  // until the next do_test_finished() call.
  do_test_pending();

  someAsyncProcess(function callback(result) {
    equal(result, expectedResult);

    // Close previous do_test_pending() call.
    do_test_finished();
  });
}
</pre>

<h4 id="Task-based_asynchronous_tests">Task-based asynchronous tests</h4>

<p>If you have many such tests to perform, it's often easier to turn them into <a href="/en-US/docs/Mozilla/JavaScript_code_modules/Task.jsm">tasks</a>, register them with <code>add_task()</code>, and invoke them with <code>run_next_test()</code>:</p>

<pre class="brush: js">function run_test() {
  run_next_test();
}

add_task(function* test_foo() {
  let foo = yield makeFoo(); // makeFoo() returns a Promise
  equal(foo, expectedFoo);
});

add_task(function* test_bar() {
  let foo = yield makeBar(); // makeBar() returns a Promise
  equal(bar, expectedBar);
});
</pre>

<h4 id="Callback-based_asynchronous_tests">Callback-based asynchronous tests</h4>

<p>Alternatively, you may put them in separate functions, register them with <code>add_test()</code>, and invoke them with <code>run_next_test()</code>:</p>

<pre class="brush: js">function run_test() {
  run_next_test();
}

add_test(function test_foo() {
  makeFoo(function callback(foo) { // makeFoo invokes a callback once completed
    equal(foo, expectedFoo);
    run_next_test();
  });
});

add_test(function test_bar() {
  makeBar(function callback(bar) {
    equal(bar, expectedBar);
    run_next_test();
  });
});
</pre>

<h3 id="Testing_under_Electrolysis">Testing under Electrolysis</h3>

<p>Since not all platforms support multi-process (electrolysis, aka "e10s"), you need to put any e10s-specific tests in a separate directory, and only run them if MOZ_IPC is defined. See /netwerk/test/Makefile.in for an example. Note that any "head_" or "tail_" scripts you have in your usual tests directory will not be run automatically in the new directory (but you can write a simple wrapper to load them: see "head_channels_clone.js" in netwerk/test/unit_ipc).</p>

<p>By default xpcshell tests run in the parent process. If you wish to run test logic in the child, you have several ways to do it:</p>

<ol>
 <li>Create a regular test_foo.js test, and then write a wrapper test_foo_wrap.js file that uses the run_test_in_child() function to run an entire script file in the child. This is an easy way to arrange for a test to be run twice, once in chrome and then later (via the _wrap.js file) in content. See /network/test/unit_ipc for examples. The run_test_in_child() function takes a callback, so you should be able to call it multiple times with different files, if that's useful.</li>
 <li>For tests that need to run logic in both the parent + child processes during a single test run, you may use the poorly documented SendCommand() function, which takes a code string to be executed on the child, and a callback function to be run on the parent when it has completed. You will want to first call do_load_child_test_harness() to set up a reasonable test environment on the child. SendCommand returns immediately, so you will generally want to use do_test_pending/do_test_finished with it. NOTE: this method of test has not been used much, and your level of pain may be significant. Consider option #1 if possible.</li>
</ol>

<p>See the documentation for run_test_in_child() and do_load_child_test_harness() in testing/xpcshell/head.js for more information.</p>

<h3 id="Platform-specific_tests">Platform-specific tests</h3>

<p>Sometimes you might want a test to know what platform it's running on (to test platform-specific features, or allow different behaviors). Unit tests are not normally invoked from a Makefile (unlike Mochitests), or preprocessed (so not #ifdefs), so platform detection with those methods isn't trivial.</p>

<h4 id="Runtime_detection">Runtime detection</h4>

<p>Some tests will want to only execute certain portions on specific platforms. One approach that's been used is to look for the existence of platform-specific components or interfaces. It's a bit hackish, but it's simple and it works.</p>

<ul>
 <li>For Windows:</li>
</ul>

<p style="margin-left: 40px;"><code>var isWindows = ("@mozilla.org/windows-registry-key;1" in Components.classes);</code></p>

<ul>
 <li>For OS X:</li>
</ul>

<p style="margin-left: 40px;"><code>var isOSX = ("nsILocalFileMac" in Components.interfaces);</code></p>

<ul>
 <li>For Linux:</li>
</ul>

<p style="margin-left: 40px;"><code>var isLinux = ("@mozilla.org/gnome-gconf-service;1" in Components.classes);</code></p>

<h3 id="Adding_your_tests_to_the_xpcshell_manifest"><strong>Adding your tests to the xpcshell manifest</strong></h3>

<p>To add a test to an xpcshell manifest file, you simply put the test file name in square brackets [] on its own line in the file, like so:</p>

<pre>#edit netwerk/test/unit/xpcshell.ini
[DEFAULT]
head = head_channels.js
tail =

[test_first.js]
</pre>

<p>If this is a new directory that doesn't have an xpcshell manifest file, create a new file named <em>xpcshell.ini</em> and then add the following to a moz.build file near that file in the directory hierarchy:</p>

<pre>XPCSHELL_TESTS_MANIFESTS += ['path/to/xpcshell.ini']</pre>

<p>Typically, the moz.build containing <em>XPCSHELL_TESTS_MANIFESTS </em>is not in the same directory as <em>xpcshell.ini</em>, but rather in a parent directory. Common directory structures look like:</p>

<ul>
 <li>feature/moz.build</li>
 <li>feature/tests/xpcshell/xpcshell.ini</li>
</ul>

<p>or</p>

<ul>
 <li>feature/moz.build</li>
 <li>feature/tests/moz.build</li>
 <li>feature/tests/xpcshell/xpcshell.ini</li>
</ul>

<h4 id="Adding_conditions_to_a_test">Adding conditions to a test</h4>

<p>Sometimes you may want to add conditions to specify that a test should be skipped in certain configurations, or that a test is known to fail on certain platforms. You can do this in xpcshell manifests by adding annotations below the test file entry in the manifest, for example:</p>

<pre>[test_first.js]
skip-if = os == 'win'
</pre>

<p>This example would skip running test_first.js on Windows. There are currently three conditionals you can specify:</p>

<h5 id="skip-if">skip-if</h5>

<p><code>skip-if</code> tells the harness to skip running this test if the condition evaluates to true. You should use this only if the test has no meaning on a certain platform, or causes undue problems like hanging the test suite for a long time.</p>

<h5 id="run-if">run-if</h5>

<p><code>run-if</code> tells the harness to only run this test if the condition evaluates to true. It functions as the inverse of <code>skip-if</code>.</p>

<h5 id="fail-if">fail-if</h5>

<p><code>fail-if</code> tells the harness that this test is expected to fail if the condition is true. If you add this to a test, make sure you file a bug on the failure and include the bug number in a comment in the manifest, like:</p>

<pre>[test_first.js]
# bug xxxxxx
fails-if = os == 'linux'</pre>

<h5 id="run-sequentially">run-sequentially</h5>

<p><code>run-sequentially </code>basically tells the harness to run the respective test in isolation. This is required for tests that are not "thread-safe". You should do all you can to avoid using this option, since this will kill performance. However, we understand that there are some cases where this is imperative, so we made this option available. If you add this to a test, make sure you specify a reason and possibly even a bug number, like:</p>

<pre>[test_first.js]
run-sequentially = Has to launch Firefox binary, bug 123456.</pre>

<h5 id="Manifest_conditional_expressions">Manifest conditional expressions</h5>

<p>For a more detailed description of the syntax of the conditional expressions, as well as what variables are available, <a href="/en/XPCshell_Test_Manifest_Expressions" title="en/XPCshell Test Manifest Expressions">see this page</a>.</p>

<h3 id="Running_unit_tests" name="Running_unit_tests">Running unit tests</h3>

<p>Tests should be run using <code>./mach xpcshell-test <var>path/to/tests/</var></code>. To run a single test, use <code>./mach xpcshell-test <var>path/to/test.js</var></code>.</p>

<h3 id="Running_unit_tests_under_a_C.2B.2B_debugger" name="Running_unit_tests_under_a_C.2B.2B_debugger">Running unit tests under a C++ debugger</h3>

<h4 id="Via_check-interactive" name="Via_check-interactive">Via <code>--debugger and -debugger-interactive</code></h4>

<p>You can specify flags when issuing the <code>xpcshell-test</code> command that will cause your test to stop right before running so you can attach to xpcshell in a debugger (implemented in {{ Bug(382682) }}).</p>

<p>Example:</p>

<pre class="eval">$ ./mach xpcshell-test --debugger gdb --debugger-interactive netwerk/test/test_resumable_channel.js
# js&gt;_execute_test();
...failure or success messages are printed to the console...
# js&gt;quit();
</pre>

<h3 id="Debugging_Electrolysis_(e10s)_xpcshell_tests">Debugging Electrolysis (e10s) xpcshell tests</h3>

<p>Under e10s you have two processes. You can debug the chrome process, the child process, or both at the same time.</p>

<h4 id="To_debug_the_chrome_process">To debug the chrome process</h4>

<p>simply use the --debugger-interactive and --debugger flags as described above.</p>

<h4 id="To_debug_the_child_process">To debug the child process</h4>

<p>The child process is where your xpcshell test's JS code is being run, so it is often--but not always--the process you're interested in. To debug the child, set MOZ_DEBUG_CHILD_PROCESS=1 in your environment (or on the command line) and run the test, and you will see the child process emit a printf with its process ID, then sleep. Attach a debugger to the child's pid, and when it wakes up you can debug it:</p>

<pre>$ MOZ_DEBUG_CHILD_PROCESS=1 ./mach xpcshell-test test_simple_wrap.js
CHILDCHILDCHILDCHILD
  debug me @13476

</pre>

<h4 id="To_debug_both_parent_and_child_processes">To debug both parent and child processes</h4>

<p>Use MOZ_DEBUG_CHILD_PROCESS=1 to attach debuggers to each process. (For gdb at least, this means running separate copies of gdb, one for each process.)</p>

<p>{{ languages( { "ja": "ja/Writing_xpcshell-based_unit_tests" } ) }}</p>

<h3 id="Common_problems">Common problems</h3>

<p>Events are not processed during test execution if not explicitly triggered. This sometimes causes issues during shutdown, when code is run that expects previously created events to have been already processed. In such cases, this code at the end of a test can help:</p>

<pre>let thread = gThreadManager.currentThread;
while (thread.hasPendingEvents())
  thread.processNextEvent(true);</pre>
